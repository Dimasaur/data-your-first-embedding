{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your first embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise objectives:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run your first RNN for NLP\n",
    "- Get a first taste of what an embedding is\n",
    "\n",
    "<hr>\n",
    "\n",
    "Words are not something you can easily feed to a Neural Network. For this reason, we have to convert them to something more meaningful. \n",
    "\n",
    "And this is exactly what _Embeddings_ are for! They map any word onto a vectorial representation (this a fancy way to represent each word with a vector ;) ). For instance, the word `dog` can be represented by the vector $(w_1, w_2, ..., w_n)$ in the embedding space, and we will learn the weights $(w_k)_k$.\n",
    "\n",
    "So let's just do it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Let's first load the data. You don't have to understand what is going on in the function, it does not matter here.\n",
    "\n",
    "⚠️ **Warning** ⚠️ The `load_data` function has a `percentage_of_sentences` argument. Depending on your computer, there are chances that too many sentences will make your compute slow down, or even freeze - your RAM can overflow. For that reason, **you should start with 10% of the sentences** and see if your computer handles it. Otherwise, rerun with a lower number. \n",
    "\n",
    "⚠️ **DISCLAIMER** ⚠️ **No need to play _who has the biggest_ (RAM) !** The idea is to get to run your models quickly to prototype. Even in real life, it is recommended that you start with a subset of your data to loop and debug quickly. So increase the number only if you are into getting the best accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "### Just run this cell to load the data ###\n",
    "###########################################\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "def load_data(percentage_of_sentences=None):\n",
    "    train_data, test_data = tfds.load(name=\"imdb_reviews\", split=[\"train\", \"test\"], batch_size=-1, as_supervised=True)\n",
    "\n",
    "    train_sentences, y_train = tfds.as_numpy(train_data)\n",
    "    test_sentences, y_test = tfds.as_numpy(test_data)\n",
    "\n",
    "    # Take only a given percentage of the entire data\n",
    "    if percentage_of_sentences is not None:\n",
    "        assert(percentage_of_sentences> 0 and percentage_of_sentences<=100)\n",
    "\n",
    "        len_train = int(percentage_of_sentences/100*len(train_sentences))\n",
    "        train_sentences, y_train = train_sentences[:len_train], y_train[:len_train]\n",
    "\n",
    "        len_test = int(percentage_of_sentences/100*len(test_sentences))\n",
    "        test_sentences, y_test = test_sentences[:len_test], y_test[:len_test]\n",
    "\n",
    "    X_train = [text_to_word_sequence(_.decode(\"utf-8\")) for _ in train_sentences]\n",
    "    X_test = [text_to_word_sequence(_.decode(\"utf-8\")) for _ in test_sentences]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_data(percentage_of_sentences=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have loaded the data, let's check it out!\n",
    "\n",
    "❓ **Question** ❓ You can play with the data here. In particular, `X_train` and `X_test` are lists of sentences. Let's print some of them, with their respective label stored in `y_train` and `y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Film rating: 0, description: when i got this movie free from my job along with three other similar movies i watched then with very low expectations now this movie isn't bad per se you get what you pay for it is a tale of love betrayal lies sex scandal everything you want in a movie definitely not a hollywood blockbuster but for cheap thrills it is not that bad i would probably never watch this movie again in a nutshell this is the kind of movie that you would see either very late at night on a local television station that is just wanting to take up some time or you would see it on a sunday afternoon on a local television station that is trying to take up some time despite the bad acting cliché lines and sub par camera work i didn't have the desire to turn off the movie and pretend like it never popped into my dvd player the story has been done many times in many movies this one is no different no better no worse br br just your average movie\n",
      "Film rating: 1, description: yes it was a little low budget but this movie shows love the only bad things about it was that you can tell the budget on this film would not compare to waterworld and though the plot was good the film never really tapped into it's full potential strong performances from everyone and the suspense makes it worthwhile to watch on a rainy night\n",
      "Film rating: 0, description: i rented end game having never heard of it but i'm fond of political thrillers so i thought i'd give it a shot after doing some research on the movie i found that it had initially been intended for theatrical release but instead had gone strait to dvd after seeing it i'm thinking no wonder the movie is shocking in its unoriginality the plot and the characters are perfunctory i figured out whodunnit by the half way mark but the ending was a curve ball i have to say i didn't expect it to end quite the way it did but that's not a point in its favor the more predictable ending would have been preferable to one that is so bad perhaps the film makers saw how predictable the film was and so they decided to throw in a twist even one that made the movie even worse br br stay away i want the 5 98 and my 107 minutes back\n",
      "Film rating: 1, description: this movie is horrible in a 'so bad it's good' kind of way br br the storyline is rehashed from so many other films of this kind that i'm not going to even bother describing it it's a sword sorcery picture has a kid hoping to realize how important he is in this world has a nomadic adventurer an evil aide sorcerer a princess a hairy creature you get the point br br the first time i caught this movie was during a very harsh winter i don't know why i decided to continue watching it for an extra five minutes before turning the channel but when i caught site of gulfax i decided to stay and watch it until the end br br gulfax is a white furry creature akin to chewbacca but not nearly as useful or entertaining to watch he looks like someone glued a bunch of white shag carpeting together and forced the actor to wear it there are scenes where it looks like the actor cannot move within or that he's almost falling over although he isn't in the movie that much the few scenes are worth it watch as he attempts to talk smack to bo svenson taking the solo chewbacca comparison's to an even higher level br br i actually bought this movie just because of that character and still have it somewhere br br gulfax may look like sh t but he made this movie the only reason i've never seen the sequel or even sought it out was because of his absence perhaps should there be a final film completing the trilogy gulfax will make a much anticipated return\n",
      "Film rating: 0, description: i believe shakespeare explained what i just read beautifully me thinks he the lady doth protest too much the whole thing sounded to me as if the author was trying to convince himself he sites profane literature writings from the same time period but not connected with the bible a number of times however i can think of at least three references off the top of my head which lend historical accuracy to events contained in the bible anyone can skew data prove anything they like but it doesn't make it true customs change word definitions change over time look at english german where it is very obviously a common root nothing stays the same it's always growing and changing the bible has many different translations but the king james version is the one i've found to be the best when it comes to any kind of research in the king james version you will notice there are certain words written in italics these words have been added by the translators and can be dropped the mean of the entire verse changes writings from around the time of christ were written without spaces without punctuation without paragraphs without numeric verses these writings look like one long word the translators added all of the above for example how would you read this godisnowhere do you read it as god is nowhere or do you read it as god is now here same string of letters two entirely different meanings this is why many biblical researchers use a 'lexicon' to assist them in translation as it provides a word for word translation from the original arabic greek or hebrew depending on the language in which the scripture was originally written it's also interesting to note that when translated into symbolic logic you can prove god exists but you can not prove he doesn't exist in the end i just love listening to people who think they are so smart that they are qualified to judge the almighty talk about ego putting soapbox away god bless maegi\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(len(X_train)):\n",
    "    if count < 5:\n",
    "        i = np.random.randint(0,len(X_train))\n",
    "        print(f\"Film rating: {y_train[i]}, description: {' '.join(X_train[i])}\")\n",
    "        count += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LABELS**: the task is a binary classification problem:\n",
    "- label 0️⃣ corresponds to a <font color=red>negative</font> movie review\n",
    "- label 1️⃣ corresponds to a <font color=green>positive</font> movie review\n",
    "\n",
    "**INPUTS**: \n",
    "- 🧹 The data has been partially cleaned! So you don't have to worry about it in this exercise. \n",
    "- ❗️ But don't forget this step in real-life challenges. \n",
    "\n",
    "Remember that words are not computer-compatible materials? You have to tokenize them!\n",
    "\n",
    "❓ **Question** ❓ Run the following cell to tokenize your sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# This initializes a Keras utilities that does all the tokenization for you\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# The tokenization learns a dictionary that maps a token (integer) to each word\n",
    "# It can be done only on the train set - we are not supposed to know the test set!\n",
    "# This tokenization also lowercases your words, apply some filters, and so on - you can check the doc if you want\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# We apply the tokenization to the train and test set\n",
    "X_train_token = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_token = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Print some of the tokenized sentences to be sure you got what you expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 1764, Tokenized words: [21, 52, 233, 1711, 15580, 2371, 37, 115, 2, 3349, 37, 47, 522, 370, 1, 287, 182, 11690, 4, 16104, 16105, 9750, 26306, 7174, 15816, 3, 1146, 7459, 9890, 24, 29, 49, 17, 1, 64, 6, 21, 52, 206]\n",
      "Index: 927, Tokenized words: [1041, 9791, 3304, 12916, 220, 160, 1222, 21697, 182, 4207, 5386, 2252, 21698, 21699, 5827, 2495, 21700, 4988, 14647, 7, 7, 3093, 310, 153, 79, 247, 5723, 6, 157, 8, 2, 2300, 449, 3, 10, 215, 197, 149, 1348, 185, 38, 368, 1831, 125, 1, 1326, 45, 5, 280, 299, 1, 2828, 16, 1, 322, 4, 23, 2694, 2205, 49, 247, 489, 274, 31, 3353, 1, 5687, 21701, 4988, 14647, 1, 663, 1549, 1591, 24, 2251, 2687, 230, 32, 328, 5, 3566, 1, 10184, 8665, 119, 646, 29, 499, 54, 5, 1, 4316, 4, 1, 439, 30, 21702, 21703, 3565, 1135, 7, 7, 11, 1660, 925, 1138, 215, 88, 1023, 10, 45, 2, 90, 15, 229, 271, 213, 72, 564, 1159, 1, 467, 1690, 181, 1488, 778, 726, 59, 303, 2, 346, 212, 470, 426, 35, 1, 439, 83, 451, 204, 81, 14176, 1085, 81, 1226, 6790, 1138, 5720, 14648, 2020, 5723, 2, 79, 375, 87, 754, 1509, 32, 1895, 2, 2200, 713, 14648, 996, 84, 241, 10, 6, 5, 1085, 8, 80, 5723, 819, 84, 5, 2981, 95, 54, 11, 18, 60, 27, 241, 4388, 5, 27, 90, 591, 16, 1, 287, 110, 115, 3093, 3, 1, 160, 10421, 2341, 2821, 463, 2761, 4, 38, 1, 882, 1643, 20, 3388, 275, 1717, 191, 5, 27, 675, 605, 9, 221, 88, 686, 30, 1, 1251, 284, 687, 11, 18, 201, 194, 1041, 9791, 3, 1969, 21704, 2002, 3565, 63, 1, 890, 5, 10627, 20, 12, 57, 13985, 3080, 4, 1, 3565, 1135, 5, 1104, 33, 2045, 775, 4, 6205, 17, 9, 271, 909, 5, 2055, 17, 20, 1, 2195, 429, 8733, 3265, 97, 35, 11, 20, 285, 381, 397, 12, 423, 83, 881]\n",
      "Index: 1340, Tokenized words: [16, 164, 124, 5, 74, 9, 867, 5, 787, 42, 2424, 4158, 14, 10, 13, 115, 5507, 15, 1, 618, 4, 10, 7, 7, 1, 816, 64, 219, 25, 274, 124, 41, 1, 5642, 312, 69, 1093, 32, 165, 544, 35, 24028, 158, 2277, 4372, 12, 2320, 289, 418, 5, 1201, 1, 1412, 312, 2812, 50, 1339, 24029, 3998, 478, 422, 364, 253, 4492, 59, 6, 1, 62, 282, 1445, 266, 173, 5, 753, 137, 11, 482, 7, 7, 1, 26, 1679, 1958, 13, 1, 1592, 15460, 24030, 14, 15461, 55, 769, 56, 659, 41, 62, 1, 378, 63, 76, 24031, 54, 3, 47, 1093, 4492, 98, 25, 76, 348, 7, 7, 1565, 30, 223, 1, 182, 3, 1033, 199, 2, 962, 1081, 5, 7388, 114, 1, 18, 13, 951]\n",
      "Index: 401, Tokenized words: [9, 13, 2317, 11, 18, 86, 4, 1, 57, 17, 9, 769, 415, 1, 538, 12, 9, 13, 155, 2, 2554, 18, 9, 934, 101, 12, 1389, 840, 2, 2955, 227, 3, 87, 131, 1224, 867, 5, 683, 8, 47, 553, 1013, 3, 1184, 10, 13, 2, 208, 392, 178, 224, 12, 245, 1, 497, 1142, 12, 3262, 8, 97, 35, 7452, 1, 3305, 39, 3040, 8, 1, 18795, 12, 10468, 1, 455, 3995, 1894, 1, 822, 121, 54, 3175, 68, 489, 43, 38, 1284, 5, 66, 1, 1700, 3, 18796, 11, 2570, 5, 18797, 3897, 1, 3995, 1894, 51, 9, 913, 11, 18, 9, 177, 10, 60, 27, 2, 533, 19, 3476, 5, 1, 2475, 3077, 711, 754, 1842, 5, 95, 2, 124, 19, 16, 290, 530, 4, 1, 272, 17, 32, 91, 32, 90, 2, 404, 19, 3, 83, 266, 1479, 1, 168, 10469, 4, 272, 441, 20, 3077, 711]\n",
      "Index: 898, Tokenized words: [1, 327, 4, 1, 351, 13, 1, 252, 18, 9, 25, 126, 113, 207, 1, 928, 14614, 7381, 341, 3954, 21590, 3, 1, 984, 108, 9, 1387, 1945, 9, 91, 101, 12, 1, 160, 3, 265, 2950, 98, 990, 722, 10, 13, 11244, 39, 21, 80, 48, 60, 27, 2719, 20, 26, 429, 4, 1, 1838, 3, 21, 1, 82, 9, 60, 1079, 11, 18, 2, 290, 42, 4, 171, 171, 115, 1, 106, 290, 115, 1, 252, 85, 2757, 140, 37, 1, 18, 5, 1, 689, 13, 61, 570, 10, 152, 22, 574, 41, 32, 69, 40, 251, 54, 15, 1479, 57, 9, 8538, 254, 36, 2942, 11, 21, 5, 66, 10, 44, 21, 301, 1, 57]\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(len(X_train_token)):\n",
    "    if count < 5:\n",
    "        i = np.random.randint(0,len(X_train_token))\n",
    "        print(f'Index: {X_train_token.index(X_train_token[i])}, Tokenized words: {X_train_token[i]}')\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary that maps each word to a token can be accessed with `tokenizer.word_index`\n",
    "    \n",
    "❓ **Question** ❓ Add a `vocab_size` variable that stores the number of different words (=tokens) in the train set. This is called the _size of the vocabulary_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your `X_train_token` and `X_test_token` contain sequences of different lengths.\n",
    "\n",
    "<img src=\"padding.png\" alt='Word2Vec' width=\"700px\" />\n",
    "\n",
    "However, a neural network has to have a tensor as input. For this reason, you have to pad your data.\n",
    "\n",
    "❓ **Question** ❓  Pad your data with the `pad_sequences` function (documentation [here](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences)). Do not forget about the `dtype` and `padding` keywords (but do not use `maxlen` here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_token_pad = pad_sequences(\n",
    "    X_train_token,\n",
    "    maxlen=None,\n",
    "    dtype='int32',\n",
    "    padding='pre',\n",
    "    truncating='pre',\n",
    "    value=0.0\n",
    ")\n",
    "\n",
    "X_test_token_pad = pad_sequences(\n",
    "    X_test_token,\n",
    "    maxlen=None,\n",
    "    dtype='int32',\n",
    "    padding='pre',\n",
    "    truncating='pre',\n",
    "    value=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112 988\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(X_train_token[1]), len(X_test_token_pad[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of before padded: 139, after padded: 988\n",
      "Length of before padded: 132, after padded: 988\n",
      "Length of before padded: 140, after padded: 988\n",
      "Length of before padded: 120, after padded: 988\n",
      "Length of before padded: 155, after padded: 988\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(len(X_train_token)):\n",
    "    if count < 5:\n",
    "        i = np.random.randint(0,len(X_train_token))\n",
    "        print(f'Length of before padded: {len(X_train_token[i])}, after padded: {len(X_test_token_pad[i])}')\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now feed this data to a Recurrent Neural Network.\n",
    "\n",
    "❓ **Question** ❓ Write a model that has:\n",
    "- an embedding layer whose `input_dim` is the size of your vocabulary (= your `vocab_size`), and whose `output_dim` is the size of the embedding space you want to have\n",
    "- a RNN (SimpleRNN, LSTM, GRU) layer\n",
    "- a Dense layer\n",
    "- an output layer\n",
    "\n",
    "⚠️ **Warning** ⚠️ Here, you don't need a masking layer. Why? Because `layers.Embedding` has a argument to do that directly, which you have to set with `mask_zero=True`. That also means that your data **HAS TO** be padded with **0** (which is the default behavior). See the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding#example_2) to understand how it **impacts** the `input_dim`.\n",
    "\n",
    "<details>\n",
    "    <summary>💡 Hint</summary>\n",
    "\n",
    "`input_dim` should equal size of vocabulary + 1\n",
    "\n",
    "</details>\n",
    "\n",
    "Compile it with the appropriate arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Normalization, Embedding\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, Flatten, LSTM\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test_token_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "def create_embedded_model():\n",
    "\n",
    "    # initializing the model\n",
    "    model = Sequential()\n",
    "\n",
    "    # adding the embedding of our dictionary\n",
    "    model.add(layers.Embedding(\n",
    "                        input_dim = vocab_size+1,\n",
    "                        input_length = len(X_train_token_pad[0]),\n",
    "                        output_dim = 20\n",
    "                 ))\n",
    "    # adding an LSTM model\n",
    "    model.add(layers.LSTM(units=20,activation='tanh'))\n",
    "\n",
    "    # adding a hidden layer\n",
    "    model.add(layers.Dense(30, activation='relu'))\n",
    "\n",
    "    # adding an output layer\n",
    "    model.add(layers.Dense(1,activation='sigmoid'))\n",
    "\n",
    "    #compiling the model\n",
    "    model.compile(\n",
    "        loss = \"binary_crossentropy\",\n",
    "        optimizer = 'adam',\n",
    "        metrics = ['accuracy']\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Look at the number of parameters in your RNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "model = create_embedded_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 1164, 20)          608400    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 20)                3280      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 30)                630       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 31        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 612,341\n",
      "Trainable params: 612,341\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Double-check that the number of parameters in your embedding layer is equal to the (number of words in your vocabulary + 1 for the masking value) $\\times$  the dimension of your embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "608400\n"
     ]
    }
   ],
   "source": [
    "manual_check = (vocab_size+1) * 20\n",
    "\n",
    "print(manual_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Start fitting your model with 20 epochs, with an early stopping criterion whose patience is equal to 4.\n",
    "\n",
    "⚠️ **Warning** ⚠️ You might see that it takes a lot of time! \n",
    "\n",
    "**So stop it after a couple of iterations!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "63/63 [==============================] - 9s 126ms/step - loss: 0.6922 - accuracy: 0.5200 - val_loss: 0.6868 - val_accuracy: 0.5920\n",
      "Epoch 2/20\n",
      "63/63 [==============================] - 7s 116ms/step - loss: 0.5855 - accuracy: 0.7455 - val_loss: 0.5355 - val_accuracy: 0.7440\n",
      "Epoch 3/20\n",
      "63/63 [==============================] - 7s 114ms/step - loss: 0.2429 - accuracy: 0.9270 - val_loss: 0.4440 - val_accuracy: 0.8080\n",
      "Epoch 4/20\n",
      "63/63 [==============================] - 7s 114ms/step - loss: 0.0640 - accuracy: 0.9835 - val_loss: 0.5058 - val_accuracy: 0.8040\n",
      "Epoch 5/20\n",
      "63/63 [==============================] - 7s 114ms/step - loss: 0.0163 - accuracy: 0.9970 - val_loss: 0.5604 - val_accuracy: 0.8060\n",
      "Epoch 6/20\n",
      "63/63 [==============================] - 7s 115ms/step - loss: 0.0065 - accuracy: 0.9990 - val_loss: 0.6700 - val_accuracy: 0.7780\n",
      "Epoch 7/20\n",
      "63/63 [==============================] - 7s 115ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.6911 - val_accuracy: 0.8020\n"
     ]
    }
   ],
   "source": [
    "# fitting the model\n",
    "\n",
    "def fit_model(model):\n",
    "\n",
    "    es = EarlyStopping(patience=4, restore_best_weights=True)\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train_token_pad,\n",
    "        y_train,\n",
    "        validation_split = 0.2,\n",
    "        batch_size = 32,\n",
    "        epochs=20,\n",
    "        callbacks = [es]\n",
    "    )\n",
    "    return history\n",
    "\n",
    "history = fit_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's not waste too much time just staring at our screen or having coffees. It is too early to start having breaks ;)\n",
    "\n",
    "❓ **Question** ❓ We will reduce the computational time. To start, let's first look at how many words there are in the different sentences of your train set (Just run the following cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_hist(X):\n",
    "    len_ = [len(_) for _ in X]\n",
    "    plt.hist(len_)\n",
    "    plt.title('Histogram of the number of sentences that have a given number of words')\n",
    "    plt.show()\n",
    "\n",
    "plot_hist(X_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will probably see that 90 to 95% of your sentences have less than 300 words. And very few have more than 1000.\n",
    "\n",
    "However, as you didn't use `maxlen` in your padding above, your input tensor has a dimension equal to the length of the sentence that has the maximum number of words.\n",
    "\n",
    "Now, let's look at how this affects the padding: \n",
    "\n",
    "\n",
    "<img src=\"tensor_size.png\" alt='Word2Vec' width=\"700px\" />\n",
    "\n",
    "Because of a few of very long sentences, one dimension of your tensor is equal to around 1000. However, most of the sentences with ~200 words have just padded values that are useless.\n",
    "\n",
    "So your tensor is mostly useless information, which still adds time to the training process.\n",
    "\n",
    "But what if you pad the data to a maximum length (`maxlen`) of say 200 (words)?\n",
    "- First, that would increase the convergence and you would not need to stare at your screen while waiting for the algorithm to converge\n",
    "- But in essence, do you really lose that much information? Do you think that you often need more than 200 words (up to 1000) to tell whether or not a sentence is positive of negative?\n",
    "\n",
    "❓ **Question** ❓ For all these reasons, re-do your padding using the `maxlen` keyword and retrain the model!  See how much faster it is now - without hurting the performance ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_token_pad_200 = pad_sequences(\n",
    "    X_train_token,\n",
    "    maxlen=200,\n",
    "    dtype='int32',\n",
    "    padding='pre',\n",
    "    truncating='pre',\n",
    "    value=0.0\n",
    ")\n",
    "\n",
    "X_test_token_pad_200 = pad_sequences(\n",
    "    X_test_token,\n",
    "    maxlen=200,\n",
    "    dtype='int32',\n",
    "    padding='pre',\n",
    "    truncating='pre',\n",
    "    value=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fit_model_200(model):\n",
    "\n",
    "    es = EarlyStopping(patience=4, restore_best_weights=True)\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train_token_pad_200,\n",
    "        y_train,\n",
    "        validation_split = 0.2,\n",
    "        batch_size = 32,\n",
    "        epochs=20,\n",
    "        callbacks = [es]\n",
    "    )\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedded_model_200():\n",
    "\n",
    "    # initializing the model\n",
    "    model = Sequential()\n",
    "\n",
    "    # adding the embedding of our dictionary\n",
    "    model.add(layers.Embedding(\n",
    "                        input_dim = vocab_size+1,\n",
    "                        input_length = 200,\n",
    "                        output_dim = 20\n",
    "                 ))\n",
    "    # adding an LSTM model\n",
    "    model.add(layers.LSTM(units=20,activation='tanh'))\n",
    "\n",
    "    # adding a hidden layer\n",
    "    model.add(layers.Dense(30, activation='relu'))\n",
    "\n",
    "    # adding an output layer\n",
    "    model.add(layers.Dense(1,activation='sigmoid'))\n",
    "\n",
    "    #compiling the model\n",
    "    model.compile(\n",
    "        loss = \"binary_crossentropy\",\n",
    "        optimizer = 'adam',\n",
    "        metrics = ['accuracy']\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "model_eff = create_embedded_model_200()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "63/63 [==============================] - 2s 22ms/step - loss: 0.6924 - accuracy: 0.5140 - val_loss: 0.6894 - val_accuracy: 0.6240\n",
      "Epoch 2/20\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.5934 - accuracy: 0.7380 - val_loss: 0.4455 - val_accuracy: 0.8040\n",
      "Epoch 3/20\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.2205 - accuracy: 0.9275 - val_loss: 0.4327 - val_accuracy: 0.8120\n",
      "Epoch 4/20\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0629 - accuracy: 0.9825 - val_loss: 0.5156 - val_accuracy: 0.7940\n",
      "Epoch 5/20\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0125 - accuracy: 0.9975 - val_loss: 0.5110 - val_accuracy: 0.8340\n",
      "Epoch 6/20\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0037 - accuracy: 0.9995 - val_loss: 0.6011 - val_accuracy: 0.8280\n",
      "Epoch 7/20\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.7614 - val_accuracy: 0.8260\n"
     ]
    }
   ],
   "source": [
    "history_eff = fit_model_200(model_eff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 1s 15ms/step - loss: 0.4628 - accuracy: 0.7976\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 0.4315 - accuracy: 0.8220\n",
      "Accuracy full length: 0.7975999712944031,-------------------Accuracy 200 length: 0.8220000267028809\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test_token_pad,y_test)\n",
    "\n",
    "results_eff = model_eff.evaluate(X_test_token_pad_200,y_test)\n",
    "\n",
    "print(f\"Accuracy full length: {results[1]},\\\n",
    "-------------------\\\n",
    "Accuracy 200 length: {results_eff[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🏁 Nice, you are now able to use `Tokenizer` and `pad_sequences`\n",
    "\n",
    "💾 Don't forget to git add/commit/push your notebook...\n",
    "\n",
    "🚀 ... and move on to the next challenge!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
